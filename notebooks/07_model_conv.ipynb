{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa82149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2364bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"../triton_model_repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0cbef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../triton_model_repository/resnet18_torch/1\")\n",
    "os.makedirs(\"../triton_model_repository/resnet18_onnx/1\")\n",
    "os.makedirs(\"../triton_model_repository/resnet18_trt_fp32/1\")\n",
    "os.makedirs(\"../triton_model_repository/resnet18_trt_fp16/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2b4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "BATCH_SIZE = 8\n",
    "PY_MODEL_PATH = './saved/resnet18.pt'\n",
    "JIT_MODEL_PATH = '../triton_model_repository/resnet18_torch/1/model.pt'\n",
    "ONNX_MODEL_PATH = '../triton_model_repository/resnet18_onnx/1/model.onnx'\n",
    "TRT_MODEL_PATH = '../triton_model_repository/resnet18_trt_fp32/1/model.plan'\n",
    "TRT_MODEL_PATH_FP16 = '../triton_model_repository/resnet18_trt_fp16/1/model.plan'\n",
    "MODEL_NAME = 'resnet18'\n",
    "NUM_CLASSES = 4\n",
    "INPUT_SHAPE = (3, 224, 224)\n",
    "CHANNEL_LAST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233eb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed9ae31",
   "metadata": {},
   "source": [
    "## Load Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a6d6796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(nn.Linear(num_ftrs,512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(512,4))\n",
    "\n",
    "if CHANNEL_LAST:\n",
    "    model = model.to(device, memory_format=torch.channels_last)\n",
    "else:\n",
    "    model = model.to(device)\n",
    "model.load_state_dict(torch.load(PY_MODEL_PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5af6c3",
   "metadata": {},
   "source": [
    "# Export to TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcfacaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHANNEL_LAST:\n",
    "    example = torch.randn((BATCH_SIZE, *INPUT_SHAPE), dtype=torch.float32, device=device).to(memory_format=torch.channels_last)\n",
    "else:\n",
    "    example = torch.randn((BATCH_SIZE, *INPUT_SHAPE), dtype=torch.float32, device=device)\n",
    "\n",
    "script = torch.jit.trace(model, example)\n",
    "script.save(JIT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd4057",
   "metadata": {},
   "source": [
    "# Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6473949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHANNEL_LAST:\n",
    "    x = torch.randn((1, *INPUT_SHAPE), dtype=torch.float32, device=device).to(memory_format=torch.channels_last)\n",
    "else:\n",
    "    x = torch.randn((1, *INPUT_SHAPE), dtype=torch.float32, device=device)\n",
    "\n",
    "torch.onnx.export(model,                       # model being run\n",
    "                  x,                           # model input (or a tuple for multiple inputs)\n",
    "                  ONNX_MODEL_PATH,             # Path to saved onnx model\n",
    "                  export_params=True,          # store the trained parameter weights inside the model file\n",
    "                  opset_version=13,            # the ONNX version to export the model to\n",
    "                  input_names = ['input'],     # the model's input names\n",
    "                  output_names = ['output'],   # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9c520",
   "metadata": {},
   "source": [
    "# Export to TensorRT\n",
    "\n",
    "TensorRT is a library that focuses specifically on running an already trained network quickly and efficiently on a GPU for high performance inference on NVIDIA GPUs. However, some pre-processing steps maybe required before converting the ONNX model to TensorRT inference engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5b8ae",
   "metadata": {},
   "source": [
    "## Using Polygraph\n",
    "\n",
    "Polygraphy is a toolkit designed to assist in running and debugging deep learning models. It can run inference among different model formats, convert models to other formats, compare performance of models, all through the comamnd-line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f9a75",
   "metadata": {},
   "source": [
    "The `surgeon sanitize` subtool can be used to fold constants in graphs and remove unused nodes. In cases where shapes are statically known, it can also simplify subgraphs involving shape operations. A simple example is shown below:\n",
    "\n",
    "Suppose you are computing, `output = input + ((a + b) + c)` where `a`, `b` and `c` are constants. By running the command given below, `polygraph` will collapse `a`, `b` and `c` into a single constant tensor, simplifying the equation to `output = input + d`.\n",
    "\n",
    "Polygraphy's surgeon tool provides a constant folding function, which is an important step for newer models before converting the ONNX model into TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8911c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this for EfficientNetV2\n",
    "#!polygraphy surgeon sanitize $ONNX_MODEL_PATH --fold-constant -o $ONNX_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a9b25",
   "metadata": {},
   "source": [
    "You can use the `run` subtool to compare the ONNX model between TensorRT and ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2adb6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored\n",
      "[I] trt-runner-N0-01/14/22-19:33:44     | Activating and starting inference\n",
      "[01/14/2022-19:33:46] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[W]     Input tensor: input (dtype=DataType.FLOAT, shape=(-1, 3, 224, 224)) | No shapes provided; Will use shape: [1, 3, 224, 224] for min/opt/max in profile.\n",
      "[W]     This will cause the tensor to have a static shape. If this is incorrect, please set the range of shapes for this input tensor.\n",
      "[I]     Configuring with profiles: [Profile().add(input, min=[1, 3, 224, 224], opt=[1, 3, 224, 224], max=[1, 3, 224, 224])]\n",
      "[I] Building engine with configuration:\n",
      "    Workspace            | 16777216 bytes (16.00 MiB)\n",
      "    Precision            | TF32: False, FP16: False, INT8: False, Strict Types: False\n",
      "    Tactic Sources       | ['CUBLAS', 'CUBLAS_LT', 'CUDNN']\n",
      "    Safety Restricted    | False\n",
      "    Profiles             | 1 profile(s)\n",
      "[I] Finished engine building in 17.097 seconds\n",
      "[I] trt-runner-N0-01/14/22-19:33:44    \n",
      "    ---- Inference Input(s) ----\n",
      "    {input [dtype=float32, shape=(1, 3, 224, 224)]}\n",
      "[I] trt-runner-N0-01/14/22-19:33:44    \n",
      "    ---- Inference Output(s) ----\n",
      "    {output [dtype=float32, shape=(1, 4)]}\n",
      "[I] trt-runner-N0-01/14/22-19:33:44     | Completed 1 iteration(s) in 3.606 ms | Average inference time: 3.606 ms.\n",
      "[I] onnxrt-runner-N0-01/14/22-19:33:44  | Activating and starting inference\n",
      "[I] onnxrt-runner-N0-01/14/22-19:33:44 \n",
      "    ---- Inference Input(s) ----\n",
      "    {input [dtype=float32, shape=(1, 3, 224, 224)]}\n",
      "[I] onnxrt-runner-N0-01/14/22-19:33:44 \n",
      "    ---- Inference Output(s) ----\n",
      "    {output [dtype=float32, shape=(1, 4)]}\n",
      "[I] onnxrt-runner-N0-01/14/22-19:33:44  | Completed 1 iteration(s) in 14.54 ms | Average inference time: 14.54 ms.\n",
      "[I] Accuracy Comparison | trt-runner-N0-01/14/22-19:33:44 vs. onnxrt-runner-N0-01/14/22-19:33:44\n",
      "[I]     Comparing Output: 'output' (dtype=float32, shape=(1, 4)) with 'output' (dtype=float32, shape=(1, 4)) | Tolerance: [abs=1e-05, rel=1e-05] | Checking elemwise error\n",
      "[I]         trt-runner-N0-01/14/22-19:33:44: output | Stats: mean=0.17267, std-dev=1.0688, var=1.1423, median=-0.045597, min=-1.0798 at (0, 1), max=1.8617 at (0, 0), avg-magnitude=0.81629\n",
      "[I]         onnxrt-runner-N0-01/14/22-19:33:44: output | Stats: mean=0.17267, std-dev=1.0688, var=1.1423, median=-0.045597, min=-1.0798 at (0, 1), max=1.8617 at (0, 0), avg-magnitude=0.81629\n",
      "[I]         Error Metrics: output\n",
      "[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0]\n",
      "[I]             Absolute Difference | Stats: mean=3.7253e-08, std-dev=4.7707e-08, var=2.276e-15, median=1.4901e-08, min=0 at (0, 1), max=1.1921e-07 at (0, 0), avg-magnitude=3.7253e-08\n",
      "[I]             Relative Difference | Stats: mean=6.6012e-08, std-dev=4.5444e-08, var=2.0651e-15, median=6.7933e-08, min=0 at (0, 1), max=1.2818e-07 at (0, 2), avg-magnitude=6.6012e-08\n",
      "[I]         PASSED | Difference is within tolerance (rel=1e-05, abs=1e-05)\n",
      "[I]     PASSED | All outputs matched | Outputs: ['output']\n",
      "[I] PASSED | Command: /opt/conda/bin/polygraphy run ../triton_model_repository/resnet50_onnx/1/model.onnx --trt --onnxrt\n"
     ]
    }
   ],
   "source": [
    "!polygraphy run $ONNX_MODEL_PATH --trt --onnxrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa28a7f",
   "metadata": {},
   "source": [
    "## Exporting to TensorRT inference engine\n",
    "\n",
    "Finally, the model is converted to TensorRT inference engine using `trtexec`, a command-line tool for working with TensorRT. The various flags used here are explained below:\n",
    "\n",
    "The `explicitBatch` flag signals to TensorRT that we will be using a fixed batch size at runtime. `minShapes` and `maxShapes`, like their name suggests, are the minimum and maximum shaped tensors that you want to pass for inferencing, while `optShapes` is the preferred shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555baa1",
   "metadata": {},
   "source": [
    "### FP32 Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f0b056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # trtexec --onnx=../triton_model_repository/resnet50_onnx/1/model.onnx --explicitBatch --workspace=16382 --optShapes=input:8x3x224x224 --maxShapes=input:128x3x224x224 --minShapes=input:1x3x224x224 --saveEngine=../triton_model_repository/resnet50_trt_fp32/1/model.plan\n",
      "[01/14/2022-19:34:05] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[01/14/2022-19:34:05] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[01/14/2022-19:34:05] [I] === Model Options ===\n",
      "[01/14/2022-19:34:05] [I] Format: ONNX\n",
      "[01/14/2022-19:34:05] [I] Model: ../triton_model_repository/resnet50_onnx/1/model.onnx\n",
      "[01/14/2022-19:34:05] [I] Output:\n",
      "[01/14/2022-19:34:05] [I] === Build Options ===\n",
      "[01/14/2022-19:34:05] [I] Max batch: explicit batch\n",
      "[01/14/2022-19:34:05] [I] Workspace: 16382 MiB\n",
      "[01/14/2022-19:34:05] [I] minTiming: 1\n",
      "[01/14/2022-19:34:05] [I] avgTiming: 8\n",
      "[01/14/2022-19:34:05] [I] Precision: FP32\n",
      "[01/14/2022-19:34:05] [I] Calibration: \n",
      "[01/14/2022-19:34:05] [I] Refit: Disabled\n",
      "[01/14/2022-19:34:05] [I] Sparsity: Disabled\n",
      "[01/14/2022-19:34:05] [I] Safe mode: Disabled\n",
      "[01/14/2022-19:34:05] [I] DirectIO mode: Disabled\n",
      "[01/14/2022-19:34:05] [I] Restricted mode: Disabled\n",
      "[01/14/2022-19:34:05] [I] Save engine: ../triton_model_repository/resnet50_trt_fp32/1/model.plan\n",
      "[01/14/2022-19:34:05] [I] Load engine: \n",
      "[01/14/2022-19:34:05] [I] Profiling verbosity: 0\n",
      "[01/14/2022-19:34:05] [I] Tactic sources: Using default tactic sources\n",
      "[01/14/2022-19:34:05] [I] timingCacheMode: local\n",
      "[01/14/2022-19:34:05] [I] timingCacheFile: \n",
      "[01/14/2022-19:34:05] [I] Input(s)s format: fp32:CHW\n",
      "[01/14/2022-19:34:05] [I] Output(s)s format: fp32:CHW\n",
      "[01/14/2022-19:34:05] [I] Input build shape: input=1x3x224x224+8x3x224x224+128x3x224x224\n",
      "[01/14/2022-19:34:05] [I] Input calibration shapes: model\n",
      "[01/14/2022-19:34:05] [I] === System Options ===\n",
      "[01/14/2022-19:34:05] [I] Device: 0\n",
      "[01/14/2022-19:34:05] [I] DLACore: \n",
      "[01/14/2022-19:34:05] [I] Plugins:\n",
      "[01/14/2022-19:34:05] [I] === Inference Options ===\n",
      "[01/14/2022-19:34:05] [I] Batch: Explicit\n",
      "[01/14/2022-19:34:05] [I] Input inference shape: input=8x3x224x224\n",
      "[01/14/2022-19:34:05] [I] Iterations: 10\n",
      "[01/14/2022-19:34:05] [I] Duration: 3s (+ 200ms warm up)\n",
      "[01/14/2022-19:34:05] [I] Sleep time: 0ms\n",
      "[01/14/2022-19:34:05] [I] Idle time: 0ms\n",
      "[01/14/2022-19:34:05] [I] Streams: 1\n",
      "[01/14/2022-19:34:05] [I] ExposeDMA: Disabled\n",
      "[01/14/2022-19:34:05] [I] Data transfers: Enabled\n",
      "[01/14/2022-19:34:05] [I] Spin-wait: Disabled\n",
      "[01/14/2022-19:34:05] [I] Multithreading: Disabled\n",
      "[01/14/2022-19:34:05] [I] CUDA Graph: Disabled\n",
      "[01/14/2022-19:34:05] [I] Separate profiling: Disabled\n",
      "[01/14/2022-19:34:05] [I] Time Deserialize: Disabled\n",
      "[01/14/2022-19:34:05] [I] Time Refit: Disabled\n",
      "[01/14/2022-19:34:05] [I] Skip inference: Disabled\n",
      "[01/14/2022-19:34:05] [I] Inputs:\n",
      "[01/14/2022-19:34:05] [I] === Reporting Options ===\n",
      "[01/14/2022-19:34:05] [I] Verbose: Disabled\n",
      "[01/14/2022-19:34:05] [I] Averages: 10 inferences\n",
      "[01/14/2022-19:34:05] [I] Percentile: 99\n",
      "[01/14/2022-19:34:05] [I] Dump refittable layers:Disabled\n",
      "[01/14/2022-19:34:05] [I] Dump output: Disabled\n",
      "[01/14/2022-19:34:05] [I] Profile: Disabled\n",
      "[01/14/2022-19:34:05] [I] Export timing to JSON file: \n",
      "[01/14/2022-19:34:05] [I] Export output to JSON file: \n",
      "[01/14/2022-19:34:05] [I] Export profile to JSON file: \n",
      "[01/14/2022-19:34:05] [I] \n",
      "[01/14/2022-19:34:05] [I] === Device Information ===\n",
      "[01/14/2022-19:34:05] [I] Selected Device: Tesla V100-DGXS-32GB\n",
      "[01/14/2022-19:34:05] [I] Compute Capability: 7.0\n",
      "[01/14/2022-19:34:05] [I] SMs: 80\n",
      "[01/14/2022-19:34:05] [I] Compute Clock Rate: 1.53 GHz\n",
      "[01/14/2022-19:34:05] [I] Device Global Memory: 32505 MiB\n",
      "[01/14/2022-19:34:05] [I] Shared Memory per SM: 96 KiB\n",
      "[01/14/2022-19:34:05] [I] Memory Bus Width: 4096 bits (ECC enabled)\n",
      "[01/14/2022-19:34:05] [I] Memory Clock Rate: 0.877 GHz\n",
      "[01/14/2022-19:34:05] [I] \n",
      "[01/14/2022-19:34:05] [I] TensorRT version: 8.2.1\n",
      "[01/14/2022-19:34:05] [I] [TRT] [MemUsageChange] Init CUDA: CPU +261, GPU +0, now: CPU 273, GPU 24069 (MiB)\n",
      "[01/14/2022-19:34:06] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 273 MiB, GPU 24069 MiB\n",
      "[01/14/2022-19:34:06] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 384 MiB, GPU 24093 MiB\n",
      "[01/14/2022-19:34:06] [I] Start parsing network model\n",
      "[01/14/2022-19:34:06] [I] [TRT] ----------------------------------------------------------------\n",
      "[01/14/2022-19:34:06] [I] [TRT] Input filename:   ../triton_model_repository/resnet50_onnx/1/model.onnx\n",
      "[01/14/2022-19:34:06] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[01/14/2022-19:34:06] [I] [TRT] Opset version:    13\n",
      "[01/14/2022-19:34:06] [I] [TRT] Producer name:    pytorch\n",
      "[01/14/2022-19:34:06] [I] [TRT] Producer version: 1.11\n",
      "[01/14/2022-19:34:06] [I] [TRT] Domain:           \n",
      "[01/14/2022-19:34:06] [I] [TRT] Model version:    0\n",
      "[01/14/2022-19:34:06] [I] [TRT] Doc string:       \n",
      "[01/14/2022-19:34:06] [I] [TRT] ----------------------------------------------------------------\n",
      "[01/14/2022-19:34:06] [W] [TRT] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[01/14/2022-19:34:06] [I] Finish parsing network model\n",
      "[01/14/2022-19:34:07] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +376, GPU +176, now: CPU 869, GPU 24275 (MiB)\n",
      "[01/14/2022-19:34:07] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +116, GPU +52, now: CPU 985, GPU 24327 (MiB)\n",
      "[01/14/2022-19:34:07] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[01/14/2022-19:34:32] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[01/14/2022-19:34:32] [I] [TRT] Total Host Persistent Memory: 121200\n",
      "[01/14/2022-19:34:32] [I] [TRT] Total Device Persistent Memory: 119239168\n",
      "[01/14/2022-19:34:32] [I] [TRT] Total Scratch Memory: 4196864\n",
      "[01/14/2022-19:34:32] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 9 MiB, GPU 3023 MiB\n",
      "[01/14/2022-19:34:32] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 3.569ms to assign 3 blocks to 71 nodes requiring 1027604480 bytes.\n",
      "[01/14/2022-19:34:32] [I] [TRT] Total Activation Memory: 1027604480\n",
      "[01/14/2022-19:34:32] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 1436, GPU 24645 (MiB)\n",
      "[01/14/2022-19:34:32] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1436, GPU 24653 (MiB)\n",
      "[01/14/2022-19:34:32] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +118, now: CPU 0, GPU 118 (MiB)\n",
      "[01/14/2022-19:34:33] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1553, GPU 24501 (MiB)\n",
      "[01/14/2022-19:34:33] [I] [TRT] Loaded engine size: 118 MiB\n",
      "[01/14/2022-19:34:33] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 1554, GPU 24629 (MiB)\n",
      "[01/14/2022-19:34:33] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1554, GPU 24637 (MiB)\n",
      "[01/14/2022-19:34:33] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +117, now: CPU 0, GPU 117 (MiB)\n",
      "[01/14/2022-19:34:33] [I] Engine built in 28.0377 sec.\n",
      "[01/14/2022-19:34:33] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 1231, GPU 24613 (MiB)\n",
      "[01/14/2022-19:34:33] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1231, GPU 24621 (MiB)\n",
      "[01/14/2022-19:34:33] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1094, now: CPU 0, GPU 1211 (MiB)\n",
      "[01/14/2022-19:34:33] [I] Using random values for input input\n",
      "[01/14/2022-19:34:33] [I] Created input binding for input with dimensions 8x3x224x224\n",
      "[01/14/2022-19:34:33] [I] Using random values for output output\n",
      "[01/14/2022-19:34:33] [I] Created output binding for output with dimensions 8x4\n",
      "[01/14/2022-19:34:33] [I] Starting inference\n",
      "[01/14/2022-19:34:36] [I] Warmup completed 32 queries over 200 ms\n",
      "[01/14/2022-19:34:36] [I] Timing trace has 472 queries over 3.01626 s\n",
      "[01/14/2022-19:34:36] [I] \n",
      "[01/14/2022-19:34:36] [I] === Trace details ===\n",
      "[01/14/2022-19:34:36] [I] Trace averages of 10 runs:\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.26329 ms - Host latency: 6.67276 ms (end to end 12.4374 ms, enqueue 0.870682 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.26995 ms - Host latency: 6.68041 ms (end to end 12.4566 ms, enqueue 0.871396 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.26514 ms - Host latency: 6.67447 ms (end to end 12.4341 ms, enqueue 0.861865 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.25326 ms - Host latency: 6.66321 ms (end to end 12.4178 ms, enqueue 0.865771 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.2679 ms - Host latency: 6.6779 ms (end to end 12.454 ms, enqueue 0.87157 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.26164 ms - Host latency: 6.67298 ms (end to end 12.4168 ms, enqueue 0.874597 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.27559 ms - Host latency: 6.69962 ms (end to end 12.417 ms, enqueue 0.838428 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.25735 ms - Host latency: 6.67921 ms (end to end 11.7744 ms, enqueue 0.833203 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.27783 ms - Host latency: 6.7026 ms (end to end 12.4689 ms, enqueue 0.820551 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.26984 ms - Host latency: 6.69745 ms (end to end 12.4499 ms, enqueue 0.840747 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.2724 ms - Host latency: 6.7013 ms (end to end 12.4542 ms, enqueue 0.880188 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.26074 ms - Host latency: 6.68879 ms (end to end 12.433 ms, enqueue 0.866724 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.26698 ms - Host latency: 6.69413 ms (end to end 12.4511 ms, enqueue 0.866425 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.26403 ms - Host latency: 6.69192 ms (end to end 12.4387 ms, enqueue 0.86864 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.27355 ms - Host latency: 6.70256 ms (end to end 12.4556 ms, enqueue 0.871143 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.2928 ms - Host latency: 6.72004 ms (end to end 11.8858 ms, enqueue 0.841492 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.28441 ms - Host latency: 6.71118 ms (end to end 12.4959 ms, enqueue 0.865332 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.30065 ms - Host latency: 6.72289 ms (end to end 12.5241 ms, enqueue 0.795459 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.3402 ms - Host latency: 6.76223 ms (end to end 12.0008 ms, enqueue 0.771924 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.31409 ms - Host latency: 6.73632 ms (end to end 12.5542 ms, enqueue 0.780542 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.29639 ms - Host latency: 6.72424 ms (end to end 12.5099 ms, enqueue 0.84563 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.27998 ms - Host latency: 6.70868 ms (end to end 12.4735 ms, enqueue 0.934021 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.2665 ms - Host latency: 6.69449 ms (end to end 12.4474 ms, enqueue 0.86709 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.27015 ms - Host latency: 6.69666 ms (end to end 12.4545 ms, enqueue 0.865979 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.2757 ms - Host latency: 6.70345 ms (end to end 12.4696 ms, enqueue 0.866357 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.28152 ms - Host latency: 6.70922 ms (end to end 12.4735 ms, enqueue 0.867029 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.26843 ms - Host latency: 6.696 ms (end to end 12.4569 ms, enqueue 0.864148 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.29021 ms - Host latency: 6.71862 ms (end to end 12.4875 ms, enqueue 0.920813 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.26177 ms - Host latency: 6.69087 ms (end to end 12.4449 ms, enqueue 0.865466 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.28853 ms - Host latency: 6.71611 ms (end to end 12.4884 ms, enqueue 0.865381 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.28857 ms - Host latency: 6.71638 ms (end to end 12.4921 ms, enqueue 0.865918 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.28225 ms - Host latency: 6.71057 ms (end to end 12.4778 ms, enqueue 0.867651 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.28542 ms - Host latency: 6.71306 ms (end to end 12.4869 ms, enqueue 0.864355 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.2793 ms - Host latency: 6.70669 ms (end to end 12.4738 ms, enqueue 0.863355 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.28425 ms - Host latency: 6.7113 ms (end to end 12.486 ms, enqueue 0.873047 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.28367 ms - Host latency: 6.71387 ms (end to end 12.4736 ms, enqueue 0.951025 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.27236 ms - Host latency: 6.69829 ms (end to end 12.4654 ms, enqueue 0.731909 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.27744 ms - Host latency: 6.70483 ms (end to end 12.4733 ms, enqueue 0.782397 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.27837 ms - Host latency: 6.70535 ms (end to end 12.467 ms, enqueue 0.864478 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.27852 ms - Host latency: 6.70803 ms (end to end 12.4642 ms, enqueue 0.866626 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.28447 ms - Host latency: 6.70977 ms (end to end 12.4078 ms, enqueue 0.820386 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.27788 ms - Host latency: 6.70249 ms (end to end 12.2635 ms, enqueue 0.793506 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.28647 ms - Host latency: 6.71077 ms (end to end 12.4845 ms, enqueue 0.808325 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.29226 ms - Host latency: 6.71965 ms (end to end 12.495 ms, enqueue 0.855664 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.29324 ms - Host latency: 6.72153 ms (end to end 12.5009 ms, enqueue 0.8729 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.2696 ms - Host latency: 6.6979 ms (end to end 12.4533 ms, enqueue 0.868262 ms)\n",
      "[01/14/2022-19:34:36] [I] Average on 10 runs - GPU latency: 6.29314 ms - Host latency: 6.72117 ms (end to end 11.8718 ms, enqueue 0.862622 ms)\n",
      "[01/14/2022-19:34:36] [I] \n",
      "[01/14/2022-19:34:36] [I] === Performance summary ===\n",
      "[01/14/2022-19:34:36] [I] Throughput: 156.485 qps\n",
      "[01/14/2022-19:34:36] [I] Latency: min = 6.62497 ms, max = 7.09619 ms, mean = 6.70404 ms, median = 6.70853 ms, percentile(99%) = 6.75635 ms\n",
      "[01/14/2022-19:34:36] [I] End-to-End Host Latency: min = 6.67542 ms, max = 12.9347 ms, mean = 12.4125 ms, median = 12.4689 ms, percentile(99%) = 12.5758 ms\n",
      "[01/14/2022-19:34:36] [I] Enqueue Time: min = 0.568604 ms, max = 1.59766 ms, mean = 0.853823 ms, median = 0.858887 ms, percentile(99%) = 1.26074 ms\n",
      "[01/14/2022-19:34:36] [I] H2D Latency: min = 0.40094 ms, max = 0.437805 ms, mean = 0.417935 ms, median = 0.419739 ms, percentile(99%) = 0.432373 ms\n",
      "[01/14/2022-19:34:36] [I] GPU Compute Time: min = 6.21582 ms, max = 6.67139 ms, mean = 6.27926 ms, median = 6.28223 ms, percentile(99%) = 6.32935 ms\n",
      "[01/14/2022-19:34:36] [I] D2H Latency: min = 0.00488281 ms, max = 0.0186768 ms, mean = 0.00684622 ms, median = 0.00695801 ms, percentile(99%) = 0.00915527 ms\n",
      "[01/14/2022-19:34:36] [I] Total Host Walltime: 3.01626 s\n",
      "[01/14/2022-19:34:36] [I] Total GPU Compute Time: 2.96381 s\n",
      "[01/14/2022-19:34:36] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[01/14/2022-19:34:36] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # trtexec --onnx=../triton_model_repository/resnet50_onnx/1/model.onnx --explicitBatch --workspace=16382 --optShapes=input:8x3x224x224 --maxShapes=input:128x3x224x224 --minShapes=input:1x3x224x224 --saveEngine=../triton_model_repository/resnet50_trt_fp32/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "!trtexec \\\n",
    "  --onnx=$ONNX_MODEL_PATH \\\n",
    "  --explicitBatch \\\n",
    "  --workspace=16382 \\\n",
    "  --optShapes=input:8x3x224x224 \\\n",
    "  --maxShapes=input:128x3x224x224 \\\n",
    "  --minShapes=input:1x3x224x224 \\\n",
    "  --saveEngine=$TRT_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07024a40",
   "metadata": {},
   "source": [
    "**if CHANNEL_LAST**\n",
    "\n",
    "\n",
    "`!trtexec \\\n",
    "  --onnx=$ONNX_MODEL_PATH \\\n",
    "  --explicitBatch \\\n",
    "  --workspace=16382 \\\n",
    "  --optShapes=input:8x224x224x3 \\\n",
    "  --maxShapes=input:128x224x224x3 \\\n",
    "  --minShapes=input:1x224x224x3 \\\n",
    "  --saveEngine=$TRT_MODEL_PATH'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a2af6",
   "metadata": {},
   "source": [
    "### FP16 Conversion\n",
    "\n",
    "As lower precision tends to run faster, we can convert the ONNX model to FP16 precision by simply passing the flag `--fp16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21c02651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # trtexec --onnx=../triton_model_repository/resnet50_onnx/1/model.onnx --explicitBatch --workspace=16382 --optShapes=input:8x3x224x224 --maxShapes=input:128x3x224x224 --minShapes=input:1x3x224x224 --saveEngine=../triton_model_repository/resnet50_trt_fp16/1/model.plan --fp16\n",
      "[01/14/2022-19:34:37] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[01/14/2022-19:34:37] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[01/14/2022-19:34:37] [I] === Model Options ===\n",
      "[01/14/2022-19:34:37] [I] Format: ONNX\n",
      "[01/14/2022-19:34:37] [I] Model: ../triton_model_repository/resnet50_onnx/1/model.onnx\n",
      "[01/14/2022-19:34:37] [I] Output:\n",
      "[01/14/2022-19:34:37] [I] === Build Options ===\n",
      "[01/14/2022-19:34:37] [I] Max batch: explicit batch\n",
      "[01/14/2022-19:34:37] [I] Workspace: 16382 MiB\n",
      "[01/14/2022-19:34:37] [I] minTiming: 1\n",
      "[01/14/2022-19:34:37] [I] avgTiming: 8\n",
      "[01/14/2022-19:34:37] [I] Precision: FP32+FP16\n",
      "[01/14/2022-19:34:37] [I] Calibration: \n",
      "[01/14/2022-19:34:37] [I] Refit: Disabled\n",
      "[01/14/2022-19:34:37] [I] Sparsity: Disabled\n",
      "[01/14/2022-19:34:37] [I] Safe mode: Disabled\n",
      "[01/14/2022-19:34:37] [I] DirectIO mode: Disabled\n",
      "[01/14/2022-19:34:37] [I] Restricted mode: Disabled\n",
      "[01/14/2022-19:34:37] [I] Save engine: ../triton_model_repository/resnet50_trt_fp16/1/model.plan\n",
      "[01/14/2022-19:34:37] [I] Load engine: \n",
      "[01/14/2022-19:34:37] [I] Profiling verbosity: 0\n",
      "[01/14/2022-19:34:37] [I] Tactic sources: Using default tactic sources\n",
      "[01/14/2022-19:34:37] [I] timingCacheMode: local\n",
      "[01/14/2022-19:34:37] [I] timingCacheFile: \n",
      "[01/14/2022-19:34:37] [I] Input(s)s format: fp32:CHW\n",
      "[01/14/2022-19:34:37] [I] Output(s)s format: fp32:CHW\n",
      "[01/14/2022-19:34:37] [I] Input build shape: input=1x3x224x224+8x3x224x224+128x3x224x224\n",
      "[01/14/2022-19:34:37] [I] Input calibration shapes: model\n",
      "[01/14/2022-19:34:37] [I] === System Options ===\n",
      "[01/14/2022-19:34:37] [I] Device: 0\n",
      "[01/14/2022-19:34:37] [I] DLACore: \n",
      "[01/14/2022-19:34:37] [I] Plugins:\n",
      "[01/14/2022-19:34:37] [I] === Inference Options ===\n",
      "[01/14/2022-19:34:37] [I] Batch: Explicit\n",
      "[01/14/2022-19:34:37] [I] Input inference shape: input=8x3x224x224\n",
      "[01/14/2022-19:34:37] [I] Iterations: 10\n",
      "[01/14/2022-19:34:37] [I] Duration: 3s (+ 200ms warm up)\n",
      "[01/14/2022-19:34:37] [I] Sleep time: 0ms\n",
      "[01/14/2022-19:34:37] [I] Idle time: 0ms\n",
      "[01/14/2022-19:34:37] [I] Streams: 1\n",
      "[01/14/2022-19:34:37] [I] ExposeDMA: Disabled\n",
      "[01/14/2022-19:34:37] [I] Data transfers: Enabled\n",
      "[01/14/2022-19:34:37] [I] Spin-wait: Disabled\n",
      "[01/14/2022-19:34:37] [I] Multithreading: Disabled\n",
      "[01/14/2022-19:34:37] [I] CUDA Graph: Disabled\n",
      "[01/14/2022-19:34:37] [I] Separate profiling: Disabled\n",
      "[01/14/2022-19:34:37] [I] Time Deserialize: Disabled\n",
      "[01/14/2022-19:34:37] [I] Time Refit: Disabled\n",
      "[01/14/2022-19:34:37] [I] Skip inference: Disabled\n",
      "[01/14/2022-19:34:37] [I] Inputs:\n",
      "[01/14/2022-19:34:37] [I] === Reporting Options ===\n",
      "[01/14/2022-19:34:37] [I] Verbose: Disabled\n",
      "[01/14/2022-19:34:37] [I] Averages: 10 inferences\n",
      "[01/14/2022-19:34:37] [I] Percentile: 99\n",
      "[01/14/2022-19:34:37] [I] Dump refittable layers:Disabled\n",
      "[01/14/2022-19:34:37] [I] Dump output: Disabled\n",
      "[01/14/2022-19:34:37] [I] Profile: Disabled\n",
      "[01/14/2022-19:34:37] [I] Export timing to JSON file: \n",
      "[01/14/2022-19:34:37] [I] Export output to JSON file: \n",
      "[01/14/2022-19:34:37] [I] Export profile to JSON file: \n",
      "[01/14/2022-19:34:37] [I] \n",
      "[01/14/2022-19:34:37] [I] === Device Information ===\n",
      "[01/14/2022-19:34:37] [I] Selected Device: Tesla V100-DGXS-32GB\n",
      "[01/14/2022-19:34:37] [I] Compute Capability: 7.0\n",
      "[01/14/2022-19:34:37] [I] SMs: 80\n",
      "[01/14/2022-19:34:37] [I] Compute Clock Rate: 1.53 GHz\n",
      "[01/14/2022-19:34:37] [I] Device Global Memory: 32505 MiB\n",
      "[01/14/2022-19:34:37] [I] Shared Memory per SM: 96 KiB\n",
      "[01/14/2022-19:34:37] [I] Memory Bus Width: 4096 bits (ECC enabled)\n",
      "[01/14/2022-19:34:37] [I] Memory Clock Rate: 0.877 GHz\n",
      "[01/14/2022-19:34:37] [I] \n",
      "[01/14/2022-19:34:37] [I] TensorRT version: 8.2.1\n",
      "[01/14/2022-19:34:38] [I] [TRT] [MemUsageChange] Init CUDA: CPU +261, GPU +0, now: CPU 273, GPU 24069 (MiB)\n",
      "[01/14/2022-19:34:38] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 273 MiB, GPU 24069 MiB\n",
      "[01/14/2022-19:34:38] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 384 MiB, GPU 24093 MiB\n",
      "[01/14/2022-19:34:38] [I] Start parsing network model\n",
      "[01/14/2022-19:34:38] [I] [TRT] ----------------------------------------------------------------\n",
      "[01/14/2022-19:34:38] [I] [TRT] Input filename:   ../triton_model_repository/resnet50_onnx/1/model.onnx\n",
      "[01/14/2022-19:34:38] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[01/14/2022-19:34:38] [I] [TRT] Opset version:    13\n",
      "[01/14/2022-19:34:38] [I] [TRT] Producer name:    pytorch\n",
      "[01/14/2022-19:34:38] [I] [TRT] Producer version: 1.11\n",
      "[01/14/2022-19:34:38] [I] [TRT] Domain:           \n",
      "[01/14/2022-19:34:38] [I] [TRT] Model version:    0\n",
      "[01/14/2022-19:34:38] [I] [TRT] Doc string:       \n",
      "[01/14/2022-19:34:38] [I] [TRT] ----------------------------------------------------------------\n",
      "[01/14/2022-19:34:39] [W] [TRT] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[01/14/2022-19:34:39] [I] Finish parsing network model\n",
      "[01/14/2022-19:34:40] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +376, GPU +176, now: CPU 869, GPU 24275 (MiB)\n",
      "[01/14/2022-19:34:40] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +116, GPU +52, now: CPU 985, GPU 24327 (MiB)\n",
      "[01/14/2022-19:34:40] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[01/14/2022-19:35:42] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[01/14/2022-19:35:42] [I] [TRT] Total Host Persistent Memory: 127904\n",
      "[01/14/2022-19:35:42] [I] [TRT] Total Device Persistent Memory: 47087104\n",
      "[01/14/2022-19:35:42] [I] [TRT] Total Scratch Memory: 2097152\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 47 MiB, GPU 3023 MiB\n",
      "[01/14/2022-19:35:42] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 2.53437ms to assign 4 blocks to 62 nodes requiring 513802241 bytes.\n",
      "[01/14/2022-19:35:42] [I] [TRT] Total Activation Memory: 513802241\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 1484, GPU 24575 (MiB)\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 1485, GPU 24583 (MiB)\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +47, GPU +47, now: CPU 47, GPU 47 (MiB)\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1484, GPU 24501 (MiB)\n",
      "[01/14/2022-19:35:42] [I] [TRT] Loaded engine size: 48 MiB\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 1485, GPU 24559 (MiB)\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1485, GPU 24567 (MiB)\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +46, now: CPU 0, GPU 46 (MiB)\n",
      "[01/14/2022-19:35:42] [I] Engine built in 64.7973 sec.\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 1232, GPU 24553 (MiB)\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1232, GPU 24561 (MiB)\n",
      "[01/14/2022-19:35:42] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +535, now: CPU 0, GPU 581 (MiB)\n",
      "[01/14/2022-19:35:42] [I] Using random values for input input\n",
      "[01/14/2022-19:35:42] [I] Created input binding for input with dimensions 8x3x224x224\n",
      "[01/14/2022-19:35:42] [I] Using random values for output output\n",
      "[01/14/2022-19:35:42] [I] Created output binding for output with dimensions 8x4\n",
      "[01/14/2022-19:35:42] [I] Starting inference\n",
      "[01/14/2022-19:35:45] [I] Warmup completed 90 queries over 200 ms\n",
      "[01/14/2022-19:35:45] [I] Timing trace has 1472 queries over 3.00657 s\n",
      "[01/14/2022-19:35:45] [I] \n",
      "[01/14/2022-19:35:45] [I] === Trace details ===\n",
      "[01/14/2022-19:35:45] [I] Trace averages of 10 runs:\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 2.04216 ms - Host latency: 2.4503 ms (end to end 4.00227 ms, enqueue 0.540622 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 2.03889 ms - Host latency: 2.44618 ms (end to end 3.99349 ms, enqueue 0.511197 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 2.03448 ms - Host latency: 2.44067 ms (end to end 3.98895 ms, enqueue 0.528481 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 2.02916 ms - Host latency: 2.43628 ms (end to end 3.97384 ms, enqueue 0.519086 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 2.03305 ms - Host latency: 2.4407 ms (end to end 3.9778 ms, enqueue 0.52468 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 2.03193 ms - Host latency: 2.43889 ms (end to end 3.98247 ms, enqueue 0.531116 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 2.03141 ms - Host latency: 2.43792 ms (end to end 3.98086 ms, enqueue 0.526938 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 2.01421 ms - Host latency: 2.42099 ms (end to end 3.7884 ms, enqueue 0.52294 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99946 ms - Host latency: 2.40773 ms (end to end 3.9172 ms, enqueue 0.501953 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99823 ms - Host latency: 2.40579 ms (end to end 3.89703 ms, enqueue 0.515009 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99987 ms - Host latency: 2.40834 ms (end to end 3.74431 ms, enqueue 0.516888 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 2.00141 ms - Host latency: 2.41517 ms (end to end 3.91446 ms, enqueue 0.509543 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99496 ms - Host latency: 2.41748 ms (end to end 3.90764 ms, enqueue 0.495767 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99843 ms - Host latency: 2.42135 ms (end to end 3.75166 ms, enqueue 0.497583 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98911 ms - Host latency: 2.4104 ms (end to end 3.52568 ms, enqueue 0.501303 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99515 ms - Host latency: 2.4157 ms (end to end 3.72709 ms, enqueue 0.484106 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99169 ms - Host latency: 2.41232 ms (end to end 3.91346 ms, enqueue 0.458362 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99404 ms - Host latency: 2.41675 ms (end to end 3.91572 ms, enqueue 0.49657 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99691 ms - Host latency: 2.41918 ms (end to end 3.92148 ms, enqueue 0.470007 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99557 ms - Host latency: 2.42 ms (end to end 3.91531 ms, enqueue 0.50108 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99485 ms - Host latency: 2.42036 ms (end to end 3.91277 ms, enqueue 0.48363 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98859 ms - Host latency: 2.41423 ms (end to end 3.89948 ms, enqueue 0.525702 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99486 ms - Host latency: 2.42132 ms (end to end 3.90855 ms, enqueue 0.529718 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98729 ms - Host latency: 2.41505 ms (end to end 3.89221 ms, enqueue 0.528741 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99393 ms - Host latency: 2.41968 ms (end to end 3.90723 ms, enqueue 0.509723 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98994 ms - Host latency: 2.41621 ms (end to end 3.89907 ms, enqueue 0.513434 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99117 ms - Host latency: 2.41678 ms (end to end 3.90321 ms, enqueue 0.507459 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9927 ms - Host latency: 2.41783 ms (end to end 3.90437 ms, enqueue 0.53139 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9879 ms - Host latency: 2.41389 ms (end to end 3.89366 ms, enqueue 0.510492 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99045 ms - Host latency: 2.41624 ms (end to end 3.90259 ms, enqueue 0.511499 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99015 ms - Host latency: 2.41663 ms (end to end 3.89751 ms, enqueue 0.511456 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9877 ms - Host latency: 2.41357 ms (end to end 3.89323 ms, enqueue 0.521191 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99148 ms - Host latency: 2.41737 ms (end to end 3.90151 ms, enqueue 0.504669 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99128 ms - Host latency: 2.41626 ms (end to end 3.8989 ms, enqueue 0.511554 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98901 ms - Host latency: 2.41437 ms (end to end 3.89643 ms, enqueue 0.525934 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9842 ms - Host latency: 2.40923 ms (end to end 3.8876 ms, enqueue 0.507324 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98891 ms - Host latency: 2.41575 ms (end to end 3.89592 ms, enqueue 0.522351 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98645 ms - Host latency: 2.41287 ms (end to end 3.89182 ms, enqueue 0.50553 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98708 ms - Host latency: 2.41262 ms (end to end 3.89065 ms, enqueue 0.513568 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9881 ms - Host latency: 2.41433 ms (end to end 3.89437 ms, enqueue 0.525543 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99095 ms - Host latency: 2.41729 ms (end to end 3.89891 ms, enqueue 0.509869 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98583 ms - Host latency: 2.41112 ms (end to end 3.88879 ms, enqueue 0.512036 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98799 ms - Host latency: 2.41469 ms (end to end 3.89001 ms, enqueue 0.523767 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98696 ms - Host latency: 2.41283 ms (end to end 3.89226 ms, enqueue 0.507251 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9866 ms - Host latency: 2.41268 ms (end to end 3.89042 ms, enqueue 0.53219 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98789 ms - Host latency: 2.41392 ms (end to end 3.8931 ms, enqueue 0.507544 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98622 ms - Host latency: 2.41329 ms (end to end 3.89047 ms, enqueue 0.535852 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98933 ms - Host latency: 2.41615 ms (end to end 3.8988 ms, enqueue 0.478857 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98811 ms - Host latency: 2.41432 ms (end to end 3.89552 ms, enqueue 0.520398 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98724 ms - Host latency: 2.41361 ms (end to end 3.89258 ms, enqueue 0.509631 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98778 ms - Host latency: 2.41277 ms (end to end 3.89514 ms, enqueue 0.528076 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98431 ms - Host latency: 2.41086 ms (end to end 3.88826 ms, enqueue 0.528357 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99146 ms - Host latency: 2.41787 ms (end to end 3.90208 ms, enqueue 0.512927 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98911 ms - Host latency: 2.41576 ms (end to end 3.89608 ms, enqueue 0.510779 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98575 ms - Host latency: 2.41215 ms (end to end 3.89221 ms, enqueue 0.512378 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98765 ms - Host latency: 2.4131 ms (end to end 3.89347 ms, enqueue 0.516272 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99008 ms - Host latency: 2.41603 ms (end to end 3.90029 ms, enqueue 0.512293 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98398 ms - Host latency: 2.40975 ms (end to end 3.8881 ms, enqueue 0.511243 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99312 ms - Host latency: 2.4197 ms (end to end 3.90137 ms, enqueue 0.515833 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9876 ms - Host latency: 2.41359 ms (end to end 3.89163 ms, enqueue 0.512097 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99403 ms - Host latency: 2.41664 ms (end to end 3.64854 ms, enqueue 0.518542 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99781 ms - Host latency: 2.4176 ms (end to end 3.93165 ms, enqueue 0.468738 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99731 ms - Host latency: 2.41581 ms (end to end 3.93079 ms, enqueue 0.491479 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99832 ms - Host latency: 2.41769 ms (end to end 3.92987 ms, enqueue 0.498022 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99423 ms - Host latency: 2.41379 ms (end to end 3.92583 ms, enqueue 0.477258 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99364 ms - Host latency: 2.41366 ms (end to end 3.92397 ms, enqueue 0.490881 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99095 ms - Host latency: 2.41202 ms (end to end 3.91338 ms, enqueue 0.501233 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99261 ms - Host latency: 2.41168 ms (end to end 3.92167 ms, enqueue 0.473035 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99231 ms - Host latency: 2.412 ms (end to end 3.92056 ms, enqueue 0.487488 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99365 ms - Host latency: 2.41259 ms (end to end 3.76853 ms, enqueue 0.50675 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99863 ms - Host latency: 2.41627 ms (end to end 3.93795 ms, enqueue 0.449182 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99948 ms - Host latency: 2.41917 ms (end to end 3.93112 ms, enqueue 0.479724 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99497 ms - Host latency: 2.41412 ms (end to end 3.92119 ms, enqueue 0.491492 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99784 ms - Host latency: 2.41871 ms (end to end 3.92678 ms, enqueue 0.490552 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 2.00211 ms - Host latency: 2.42053 ms (end to end 3.93766 ms, enqueue 0.460437 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99701 ms - Host latency: 2.41686 ms (end to end 3.92676 ms, enqueue 0.484094 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99926 ms - Host latency: 2.41968 ms (end to end 3.93191 ms, enqueue 0.475977 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9938 ms - Host latency: 2.41621 ms (end to end 3.91489 ms, enqueue 0.508594 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99877 ms - Host latency: 2.42178 ms (end to end 3.92617 ms, enqueue 0.497925 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99115 ms - Host latency: 2.41709 ms (end to end 3.90929 ms, enqueue 0.514539 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99498 ms - Host latency: 2.42242 ms (end to end 3.91254 ms, enqueue 0.533618 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99587 ms - Host latency: 2.4234 ms (end to end 3.91515 ms, enqueue 0.517932 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99088 ms - Host latency: 2.4182 ms (end to end 3.90576 ms, enqueue 0.513049 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99127 ms - Host latency: 2.41776 ms (end to end 3.90493 ms, enqueue 0.516003 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99672 ms - Host latency: 2.42257 ms (end to end 3.91636 ms, enqueue 0.518604 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9968 ms - Host latency: 2.42339 ms (end to end 3.9191 ms, enqueue 0.524365 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99086 ms - Host latency: 2.4176 ms (end to end 3.90645 ms, enqueue 0.512268 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99406 ms - Host latency: 2.41971 ms (end to end 3.91047 ms, enqueue 0.520288 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99191 ms - Host latency: 2.41703 ms (end to end 3.91001 ms, enqueue 0.519006 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99519 ms - Host latency: 2.42039 ms (end to end 3.91489 ms, enqueue 0.513843 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99095 ms - Host latency: 2.41752 ms (end to end 3.90767 ms, enqueue 0.52041 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98831 ms - Host latency: 2.41443 ms (end to end 3.90095 ms, enqueue 0.51897 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98977 ms - Host latency: 2.41592 ms (end to end 3.90288 ms, enqueue 0.514722 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9864 ms - Host latency: 2.41284 ms (end to end 3.89197 ms, enqueue 0.512305 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98899 ms - Host latency: 2.41599 ms (end to end 3.90044 ms, enqueue 0.515991 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99036 ms - Host latency: 2.41602 ms (end to end 3.90479 ms, enqueue 0.523022 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99299 ms - Host latency: 2.41868 ms (end to end 3.91216 ms, enqueue 0.511304 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99175 ms - Host latency: 2.41797 ms (end to end 3.90596 ms, enqueue 0.510352 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98821 ms - Host latency: 2.41431 ms (end to end 3.9001 ms, enqueue 0.51311 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99407 ms - Host latency: 2.42004 ms (end to end 3.90884 ms, enqueue 0.519312 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99358 ms - Host latency: 2.41895 ms (end to end 3.91077 ms, enqueue 0.507129 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98999 ms - Host latency: 2.41565 ms (end to end 3.90039 ms, enqueue 0.511304 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99124 ms - Host latency: 2.41821 ms (end to end 3.90398 ms, enqueue 0.513037 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98743 ms - Host latency: 2.41448 ms (end to end 3.89644 ms, enqueue 0.523193 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98843 ms - Host latency: 2.41375 ms (end to end 3.89753 ms, enqueue 0.50957 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98945 ms - Host latency: 2.41472 ms (end to end 3.90037 ms, enqueue 0.514429 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98999 ms - Host latency: 2.41675 ms (end to end 3.8999 ms, enqueue 0.523975 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98699 ms - Host latency: 2.41421 ms (end to end 3.89685 ms, enqueue 0.506567 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99045 ms - Host latency: 2.41614 ms (end to end 3.89829 ms, enqueue 0.523511 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99094 ms - Host latency: 2.41643 ms (end to end 3.90381 ms, enqueue 0.507471 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99131 ms - Host latency: 2.41738 ms (end to end 3.9041 ms, enqueue 0.515503 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99038 ms - Host latency: 2.41628 ms (end to end 3.90117 ms, enqueue 0.525317 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99255 ms - Host latency: 2.41892 ms (end to end 3.9061 ms, enqueue 0.507153 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98831 ms - Host latency: 2.41458 ms (end to end 3.89785 ms, enqueue 0.517603 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9876 ms - Host latency: 2.4145 ms (end to end 3.89324 ms, enqueue 0.539453 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99146 ms - Host latency: 2.41868 ms (end to end 3.90254 ms, enqueue 0.472412 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98655 ms - Host latency: 2.41384 ms (end to end 3.80259 ms, enqueue 0.499072 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99165 ms - Host latency: 2.41775 ms (end to end 3.90198 ms, enqueue 0.5146 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98611 ms - Host latency: 2.41067 ms (end to end 3.89304 ms, enqueue 0.485205 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99109 ms - Host latency: 2.41428 ms (end to end 3.48821 ms, enqueue 0.504858 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98545 ms - Host latency: 2.40947 ms (end to end 3.76128 ms, enqueue 0.510791 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98181 ms - Host latency: 2.40232 ms (end to end 3.55491 ms, enqueue 0.491675 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98933 ms - Host latency: 2.40867 ms (end to end 3.87395 ms, enqueue 0.489404 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99048 ms - Host latency: 2.41108 ms (end to end 3.90588 ms, enqueue 0.484448 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98928 ms - Host latency: 2.4092 ms (end to end 3.905 ms, enqueue 0.494116 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98855 ms - Host latency: 2.40974 ms (end to end 3.90239 ms, enqueue 0.491602 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98962 ms - Host latency: 2.41428 ms (end to end 3.90288 ms, enqueue 0.496191 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98691 ms - Host latency: 2.40972 ms (end to end 3.89775 ms, enqueue 0.494043 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9884 ms - Host latency: 2.41343 ms (end to end 3.8979 ms, enqueue 0.501855 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99617 ms - Host latency: 2.42327 ms (end to end 3.91072 ms, enqueue 0.51582 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98811 ms - Host latency: 2.41423 ms (end to end 3.89775 ms, enqueue 0.507788 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98689 ms - Host latency: 2.41184 ms (end to end 3.89021 ms, enqueue 0.519165 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98945 ms - Host latency: 2.41597 ms (end to end 3.89653 ms, enqueue 0.510938 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99543 ms - Host latency: 2.421 ms (end to end 3.90793 ms, enqueue 0.515625 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.9887 ms - Host latency: 2.41541 ms (end to end 3.89553 ms, enqueue 0.518994 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99048 ms - Host latency: 2.41602 ms (end to end 3.89597 ms, enqueue 0.513672 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98462 ms - Host latency: 2.40979 ms (end to end 3.88796 ms, enqueue 0.517114 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98831 ms - Host latency: 2.41655 ms (end to end 3.8925 ms, enqueue 0.517261 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98813 ms - Host latency: 2.41396 ms (end to end 3.89426 ms, enqueue 0.517529 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99417 ms - Host latency: 2.41958 ms (end to end 3.90408 ms, enqueue 0.514624 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98738 ms - Host latency: 2.41433 ms (end to end 3.89241 ms, enqueue 0.513232 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98994 ms - Host latency: 2.41597 ms (end to end 3.89756 ms, enqueue 0.516724 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.99177 ms - Host latency: 2.41802 ms (end to end 3.89971 ms, enqueue 0.511572 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98999 ms - Host latency: 2.41519 ms (end to end 3.89917 ms, enqueue 0.515332 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98972 ms - Host latency: 2.4175 ms (end to end 3.89614 ms, enqueue 0.514551 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98694 ms - Host latency: 2.41284 ms (end to end 3.89292 ms, enqueue 0.515259 ms)\n",
      "[01/14/2022-19:35:45] [I] Average on 10 runs - GPU latency: 1.98687 ms - Host latency: 2.41211 ms (end to end 3.89214 ms, enqueue 0.517725 ms)\n",
      "[01/14/2022-19:35:45] [I] \n",
      "[01/14/2022-19:35:45] [I] === Performance summary ===\n",
      "[01/14/2022-19:35:45] [I] Throughput: 489.595 qps\n",
      "[01/14/2022-19:35:45] [I] Latency: min = 2.37842 ms, max = 2.46156 ms, mean = 2.41668 ms, median = 2.41592 ms, percentile(99%) = 2.44672 ms\n",
      "[01/14/2022-19:35:45] [I] End-to-End Host Latency: min = 2.40283 ms, max = 4.02013 ms, mean = 3.89157 ms, median = 3.90308 ms, percentile(99%) = 3.99643 ms\n",
      "[01/14/2022-19:35:45] [I] Enqueue Time: min = 0.413818 ms, max = 0.616455 ms, mean = 0.508973 ms, median = 0.505615 ms, percentile(99%) = 0.567627 ms\n",
      "[01/14/2022-19:35:45] [I] H2D Latency: min = 0.398682 ms, max = 0.437866 ms, mean = 0.416879 ms, median = 0.418396 ms, percentile(99%) = 0.42804 ms\n",
      "[01/14/2022-19:35:45] [I] GPU Compute Time: min = 1.95691 ms, max = 2.05005 ms, mean = 1.99332 ms, median = 1.9917 ms, percentile(99%) = 2.04083 ms\n",
      "[01/14/2022-19:35:45] [I] D2H Latency: min = 0.00488281 ms, max = 0.017395 ms, mean = 0.00649143 ms, median = 0.0065918 ms, percentile(99%) = 0.00830078 ms\n",
      "[01/14/2022-19:35:45] [I] Total Host Walltime: 3.00657 s\n",
      "[01/14/2022-19:35:45] [I] Total GPU Compute Time: 2.93416 s\n",
      "[01/14/2022-19:35:45] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[01/14/2022-19:35:45] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # trtexec --onnx=../triton_model_repository/resnet50_onnx/1/model.onnx --explicitBatch --workspace=16382 --optShapes=input:8x3x224x224 --maxShapes=input:128x3x224x224 --minShapes=input:1x3x224x224 --saveEngine=../triton_model_repository/resnet50_trt_fp16/1/model.plan --fp16\n"
     ]
    }
   ],
   "source": [
    "!trtexec \\\n",
    "  --onnx=$ONNX_MODEL_PATH \\\n",
    "  --explicitBatch \\\n",
    "  --workspace=16382 \\\n",
    "  --optShapes=input:8x3x224x224 \\\n",
    "  --maxShapes=input:128x3x224x224 \\\n",
    "  --minShapes=input:1x3x224x224 \\\n",
    "  --saveEngine=$TRT_MODEL_PATH_FP16 --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb4d11",
   "metadata": {},
   "source": [
    "**if CHANNEL_LAST**\n",
    "\n",
    "\n",
    "`!trtexec \\\n",
    "  --onnx=$ONNX_MODEL_PATH \\\n",
    "  --explicitBatch \\\n",
    "  --workspace=16382 \\\n",
    "  --optShapes=input:8x224x224x3 \\\n",
    "  --maxShapes=input:128x224x224x3 \\\n",
    "  --minShapes=input:1x224x224x3 \\\n",
    "  --saveEngine=$TRT_MODEL_PATH' --fp16`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928321f",
   "metadata": {},
   "source": [
    "Test the TensorRT model for dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1347fa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # trtexec --loadEngine=../triton_model_repository/resnet50_trt_fp32/1/model.plan --shapes=input:8x3x224x224\n",
      "[01/14/2022-19:35:47] [I] === Model Options ===\n",
      "[01/14/2022-19:35:47] [I] Format: *\n",
      "[01/14/2022-19:35:47] [I] Model: \n",
      "[01/14/2022-19:35:47] [I] Output:\n",
      "[01/14/2022-19:35:47] [I] === Build Options ===\n",
      "[01/14/2022-19:35:47] [I] Max batch: explicit batch\n",
      "[01/14/2022-19:35:47] [I] Workspace: 16 MiB\n",
      "[01/14/2022-19:35:47] [I] minTiming: 1\n",
      "[01/14/2022-19:35:47] [I] avgTiming: 8\n",
      "[01/14/2022-19:35:47] [I] Precision: FP32\n",
      "[01/14/2022-19:35:47] [I] Calibration: \n",
      "[01/14/2022-19:35:47] [I] Refit: Disabled\n",
      "[01/14/2022-19:35:47] [I] Sparsity: Disabled\n",
      "[01/14/2022-19:35:47] [I] Safe mode: Disabled\n",
      "[01/14/2022-19:35:47] [I] DirectIO mode: Disabled\n",
      "[01/14/2022-19:35:47] [I] Restricted mode: Disabled\n",
      "[01/14/2022-19:35:47] [I] Save engine: \n",
      "[01/14/2022-19:35:47] [I] Load engine: ../triton_model_repository/resnet50_trt_fp32/1/model.plan\n",
      "[01/14/2022-19:35:47] [I] Profiling verbosity: 0\n",
      "[01/14/2022-19:35:47] [I] Tactic sources: Using default tactic sources\n",
      "[01/14/2022-19:35:47] [I] timingCacheMode: local\n",
      "[01/14/2022-19:35:47] [I] timingCacheFile: \n",
      "[01/14/2022-19:35:47] [I] Input(s)s format: fp32:CHW\n",
      "[01/14/2022-19:35:47] [I] Output(s)s format: fp32:CHW\n",
      "[01/14/2022-19:35:47] [I] Input build shape: input=8x3x224x224+8x3x224x224+8x3x224x224\n",
      "[01/14/2022-19:35:47] [I] Input calibration shapes: model\n",
      "[01/14/2022-19:35:47] [I] === System Options ===\n",
      "[01/14/2022-19:35:47] [I] Device: 0\n",
      "[01/14/2022-19:35:47] [I] DLACore: \n",
      "[01/14/2022-19:35:47] [I] Plugins:\n",
      "[01/14/2022-19:35:47] [I] === Inference Options ===\n",
      "[01/14/2022-19:35:47] [I] Batch: Explicit\n",
      "[01/14/2022-19:35:47] [I] Input inference shape: input=8x3x224x224\n",
      "[01/14/2022-19:35:47] [I] Iterations: 10\n",
      "[01/14/2022-19:35:47] [I] Duration: 3s (+ 200ms warm up)\n",
      "[01/14/2022-19:35:47] [I] Sleep time: 0ms\n",
      "[01/14/2022-19:35:47] [I] Idle time: 0ms\n",
      "[01/14/2022-19:35:47] [I] Streams: 1\n",
      "[01/14/2022-19:35:47] [I] ExposeDMA: Disabled\n",
      "[01/14/2022-19:35:47] [I] Data transfers: Enabled\n",
      "[01/14/2022-19:35:47] [I] Spin-wait: Disabled\n",
      "[01/14/2022-19:35:47] [I] Multithreading: Disabled\n",
      "[01/14/2022-19:35:47] [I] CUDA Graph: Disabled\n",
      "[01/14/2022-19:35:47] [I] Separate profiling: Disabled\n",
      "[01/14/2022-19:35:47] [I] Time Deserialize: Disabled\n",
      "[01/14/2022-19:35:47] [I] Time Refit: Disabled\n",
      "[01/14/2022-19:35:47] [I] Skip inference: Disabled\n",
      "[01/14/2022-19:35:47] [I] Inputs:\n",
      "[01/14/2022-19:35:47] [I] === Reporting Options ===\n",
      "[01/14/2022-19:35:47] [I] Verbose: Disabled\n",
      "[01/14/2022-19:35:47] [I] Averages: 10 inferences\n",
      "[01/14/2022-19:35:47] [I] Percentile: 99\n",
      "[01/14/2022-19:35:47] [I] Dump refittable layers:Disabled\n",
      "[01/14/2022-19:35:47] [I] Dump output: Disabled\n",
      "[01/14/2022-19:35:47] [I] Profile: Disabled\n",
      "[01/14/2022-19:35:47] [I] Export timing to JSON file: \n",
      "[01/14/2022-19:35:47] [I] Export output to JSON file: \n",
      "[01/14/2022-19:35:47] [I] Export profile to JSON file: \n",
      "[01/14/2022-19:35:47] [I] \n",
      "[01/14/2022-19:35:47] [I] === Device Information ===\n",
      "[01/14/2022-19:35:47] [I] Selected Device: Tesla V100-DGXS-32GB\n",
      "[01/14/2022-19:35:47] [I] Compute Capability: 7.0\n",
      "[01/14/2022-19:35:47] [I] SMs: 80\n",
      "[01/14/2022-19:35:47] [I] Compute Clock Rate: 1.53 GHz\n",
      "[01/14/2022-19:35:47] [I] Device Global Memory: 32505 MiB\n",
      "[01/14/2022-19:35:47] [I] Shared Memory per SM: 96 KiB\n",
      "[01/14/2022-19:35:47] [I] Memory Bus Width: 4096 bits (ECC enabled)\n",
      "[01/14/2022-19:35:47] [I] Memory Clock Rate: 0.877 GHz\n",
      "[01/14/2022-19:35:47] [I] \n",
      "[01/14/2022-19:35:47] [I] TensorRT version: 8.2.1\n",
      "[01/14/2022-19:35:47] [I] [TRT] [MemUsageChange] Init CUDA: CPU +261, GPU +0, now: CPU 391, GPU 24069 (MiB)\n",
      "[01/14/2022-19:35:47] [I] [TRT] Loaded engine size: 118 MiB\n",
      "[01/14/2022-19:35:48] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +376, GPU +176, now: CPU 768, GPU 24363 (MiB)\n",
      "[01/14/2022-19:35:48] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +116, GPU +54, now: CPU 884, GPU 24417 (MiB)\n",
      "[01/14/2022-19:35:48] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +117, now: CPU 0, GPU 117 (MiB)\n",
      "[01/14/2022-19:35:48] [I] Engine loaded in 1.41889 sec.\n",
      "[01/14/2022-19:35:48] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +10, now: CPU 766, GPU 24409 (MiB)\n",
      "[01/14/2022-19:35:48] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 766, GPU 24417 (MiB)\n",
      "[01/14/2022-19:35:48] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1094, now: CPU 0, GPU 1211 (MiB)\n",
      "[01/14/2022-19:35:48] [I] Using random values for input input\n",
      "[01/14/2022-19:35:48] [I] Created input binding for input with dimensions 8x3x224x224\n",
      "[01/14/2022-19:35:48] [I] Using random values for output output\n",
      "[01/14/2022-19:35:48] [I] Created output binding for output with dimensions 8x4\n",
      "[01/14/2022-19:35:48] [I] Starting inference\n",
      "[01/14/2022-19:35:51] [I] Warmup completed 32 queries over 200 ms\n",
      "[01/14/2022-19:35:51] [I] Timing trace has 471 queries over 3.01538 s\n",
      "[01/14/2022-19:35:51] [I] \n",
      "[01/14/2022-19:35:51] [I] === Trace details ===\n",
      "[01/14/2022-19:35:51] [I] Trace averages of 10 runs:\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29781 ms - Host latency: 6.70795 ms (end to end 12.495 ms, enqueue 0.88042 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29688 ms - Host latency: 6.70864 ms (end to end 12.5004 ms, enqueue 0.880939 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28122 ms - Host latency: 6.69121 ms (end to end 12.4714 ms, enqueue 0.878445 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29678 ms - Host latency: 6.70704 ms (end to end 12.4985 ms, enqueue 0.884119 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28418 ms - Host latency: 6.69437 ms (end to end 12.4725 ms, enqueue 0.883157 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28082 ms - Host latency: 6.69091 ms (end to end 12.4644 ms, enqueue 0.88172 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.30037 ms - Host latency: 6.70995 ms (end to end 11.8943 ms, enqueue 0.864581 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.31082 ms - Host latency: 6.72144 ms (end to end 12.5245 ms, enqueue 0.86983 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.31738 ms - Host latency: 6.72908 ms (end to end 12.5389 ms, enqueue 0.881879 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.30783 ms - Host latency: 6.71757 ms (end to end 12.537 ms, enqueue 0.87489 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28787 ms - Host latency: 6.70135 ms (end to end 12.4695 ms, enqueue 0.890668 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28929 ms - Host latency: 6.71691 ms (end to end 12.4342 ms, enqueue 0.844629 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28243 ms - Host latency: 6.7045 ms (end to end 11.2431 ms, enqueue 0.810895 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29149 ms - Host latency: 6.7171 ms (end to end 12.4853 ms, enqueue 0.849866 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29338 ms - Host latency: 6.72157 ms (end to end 12.4922 ms, enqueue 0.881689 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29023 ms - Host latency: 6.71958 ms (end to end 12.4761 ms, enqueue 0.880884 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28009 ms - Host latency: 6.708 ms (end to end 12.4704 ms, enqueue 0.88396 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28888 ms - Host latency: 6.71627 ms (end to end 12.4796 ms, enqueue 0.875623 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28416 ms - Host latency: 6.71287 ms (end to end 12.476 ms, enqueue 0.891919 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28868 ms - Host latency: 6.71613 ms (end to end 12.4542 ms, enqueue 0.8771 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29944 ms - Host latency: 6.72803 ms (end to end 12.5019 ms, enqueue 0.880151 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29813 ms - Host latency: 6.72727 ms (end to end 12.4994 ms, enqueue 0.877869 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29534 ms - Host latency: 6.72317 ms (end to end 12.4916 ms, enqueue 0.882031 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29174 ms - Host latency: 6.71996 ms (end to end 12.4868 ms, enqueue 0.884045 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28663 ms - Host latency: 6.71559 ms (end to end 12.478 ms, enqueue 0.884204 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29507 ms - Host latency: 6.72354 ms (end to end 12.4915 ms, enqueue 0.877222 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29546 ms - Host latency: 6.72424 ms (end to end 12.4979 ms, enqueue 0.886462 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29956 ms - Host latency: 6.72847 ms (end to end 12.4974 ms, enqueue 0.893115 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.28806 ms - Host latency: 6.71642 ms (end to end 12.4789 ms, enqueue 0.885181 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29354 ms - Host latency: 6.72184 ms (end to end 12.4897 ms, enqueue 0.895447 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.30835 ms - Host latency: 6.7373 ms (end to end 12.5182 ms, enqueue 0.880273 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.32351 ms - Host latency: 6.7501 ms (end to end 11.5458 ms, enqueue 0.873804 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.32344 ms - Host latency: 6.75151 ms (end to end 12.5541 ms, enqueue 0.872046 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.32336 ms - Host latency: 6.75161 ms (end to end 12.558 ms, enqueue 0.88042 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.30264 ms - Host latency: 6.73208 ms (end to end 12.5208 ms, enqueue 0.887793 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.30432 ms - Host latency: 6.73298 ms (end to end 12.5137 ms, enqueue 0.891016 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.31091 ms - Host latency: 6.73818 ms (end to end 12.5334 ms, enqueue 0.879663 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.32085 ms - Host latency: 6.75112 ms (end to end 12.5231 ms, enqueue 0.890991 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.3093 ms - Host latency: 6.7385 ms (end to end 12.5128 ms, enqueue 0.887012 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.30002 ms - Host latency: 6.72791 ms (end to end 12.5047 ms, enqueue 0.855688 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29102 ms - Host latency: 6.71978 ms (end to end 12.4816 ms, enqueue 0.881104 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.31606 ms - Host latency: 6.74316 ms (end to end 11.4414 ms, enqueue 0.877979 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.32014 ms - Host latency: 6.74836 ms (end to end 12.546 ms, enqueue 0.882056 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.31443 ms - Host latency: 6.74224 ms (end to end 12.541 ms, enqueue 0.881689 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.30876 ms - Host latency: 6.73721 ms (end to end 12.5123 ms, enqueue 0.881274 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.29634 ms - Host latency: 6.72339 ms (end to end 12.4691 ms, enqueue 0.855322 ms)\n",
      "[01/14/2022-19:35:51] [I] Average on 10 runs - GPU latency: 6.3022 ms - Host latency: 6.72446 ms (end to end 11.2735 ms, enqueue 0.808081 ms)\n",
      "[01/14/2022-19:35:51] [I] \n",
      "[01/14/2022-19:35:51] [I] === Performance summary ===\n",
      "[01/14/2022-19:35:51] [I] Throughput: 156.199 qps\n",
      "[01/14/2022-19:35:51] [I] Latency: min = 6.63666 ms, max = 6.78931 ms, mean = 6.72317 ms, median = 6.72418 ms, percentile(99%) = 6.77881 ms\n",
      "[01/14/2022-19:35:51] [I] End-to-End Host Latency: min = 6.69806 ms, max = 12.6326 ms, mean = 12.3904 ms, median = 12.5 ms, percentile(99%) = 12.5957 ms\n",
      "[01/14/2022-19:35:51] [I] Enqueue Time: min = 0.75708 ms, max = 0.968628 ms, mean = 0.876021 ms, median = 0.875732 ms, percentile(99%) = 0.933228 ms\n",
      "[01/14/2022-19:35:51] [I] H2D Latency: min = 0.39856 ms, max = 0.435486 ms, mean = 0.416794 ms, median = 0.420166 ms, percentile(99%) = 0.428833 ms\n",
      "[01/14/2022-19:35:51] [I] GPU Compute Time: min = 6.22797 ms, max = 6.3775 ms, mean = 6.29931 ms, median = 6.30069 ms, percentile(99%) = 6.35278 ms\n",
      "[01/14/2022-19:35:51] [I] D2H Latency: min = 0.00488281 ms, max = 0.00976562 ms, mean = 0.00706709 ms, median = 0.00720215 ms, percentile(99%) = 0.00927734 ms\n",
      "[01/14/2022-19:35:51] [I] Total Host Walltime: 3.01538 s\n",
      "[01/14/2022-19:35:51] [I] Total GPU Compute Time: 2.96697 s\n",
      "[01/14/2022-19:35:51] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[01/14/2022-19:35:51] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # trtexec --loadEngine=../triton_model_repository/resnet50_trt_fp32/1/model.plan --shapes=input:8x3x224x224\n"
     ]
    }
   ],
   "source": [
    "!trtexec --loadEngine=$TRT_MODEL_PATH --shapes=input:8x3x224x224"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
