## Model Conversion

 - Firstly, you will need to convert the model to ONNX format. This can be done easily using `torch.onnx.export` by passing in dummy data. One should be careful here to specify the dynamic axes for batch size.

- Next, we run some pre-processing on the ONNX model using `Polygraph` which helps us fold constants in a graph and remove unused nodes. This step is essential while converting newer models to TensorRT.

- Finally, using `trtexec`, a command line tool for working with TensorRT, we convert the ONNX model to an engine file.

This process in explained in-depth in [model_conv.ipynb](../notebooks/model_conv.ipynb). Here, the PyTorch model is also converted to TorchScript which allows you to serialize your models in a way that it can be loaded in non-Python environments and run in an optimised manner. This can be done in 2 ways:

- Tracing: We pass an example input to the model and it records the operations that occur when the network ran, creating a graph for the same.
- Scripting: It performs direct analysis of the Python source code to transform it into TorchScript.

A more in-depth tutorial regarding TorchScript can be found [here](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html).


## Model Inferencing

Once you have converted the models to these different frameworks, we need to compare how well they perform. For this purpose, we run inference on 
- PyTorch: [pytorch_inference.ipynb](../notebooks/pytorch_inference.ipynb)
- ONNX: [onnx_inference.ipynb](../notebooks/onnx_inference.ipynb)
- TensorRT: [trt_inference.ipynb](../notebooks/trt_inference.ipynb)

Inference on PyTorch model can be easily done. However, for ONNX and TensorRT, we utilise ONNX Runtine and TRT Runtime to perform inferencing.