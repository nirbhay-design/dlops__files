## INT8 Quantisation

Most of the models are trained in floating-point 32-bit arithmetic to take advantage of a wider dynamic range. However, at inference, these models may take a longer time to predict results compared to reduced precision inference, causing some delay in the real-time responses, and affecting the user experience. Itâ€™s better in many cases to use reduced precision or 8-bit integer numbers. The challenge is that simply rounding the weights after training may result in a lower accuracy model, especially if the weights have a wide dynamic range.

There are two methods which are used to convert a model to 8-bit integer precision. Today we will use one of them called:-

### Post Training Quantisation (PTQ)

Like the name suggests, in this approach, the quantization takes place only after the model has finished training. To perform this, simply use `trtexec` and INT8 precision flag to convert your model. An example has been shown below:

```bash
trtexec \
  --explicitBatch \
  --workspace=16382 \
  --optShapes=input:8x3x224x224 \
  --maxShapes=input:128x3x224x224 \
  --onnx=./model.onnx --saveEngine=./model.plan --int8
```

Once converted to ONNX, you can use the `trtexec` as before to convert the model to TensorRT engine. 