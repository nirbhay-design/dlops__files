## Triton Inference Server

Triton Inference Server provides an inferencing solution for deep learning models to be easily deployed and integrated with various functionalities. It supports HTTP and gRPC protocol that allows clients to request for inferencing, utilising any model of choice being managed by the server. 

### Creating Model Repository

The first step in using Triton is to place your models into a model repository, whose structure is as follows:

```bash
model_dir/
├── model_name_1                
│   ├── 1
│   │   └── model.pt
│   ├── 2
│   │   └── model.pt
│   ├── config.pbtxt
│   └── labels.txt
└── model_name_2                
    ├── 1
    │   └── model.onnx
    ├── 2
    │   └── model.onnx
    ├── config.pbtxt
    └── labels.txt
```

Let's unpack what all is going on in here. First of all, within the `model_dir`, there should be atleast one sub-directory which contains the information of a model. For each model, say `model_name_1`, there should be numeric sub-directories representing the version of the model. The subdirectories that are not numerically named, or have names that start with 0 will be ignored. Finally, in this numerically named directory, you will store the model. For PyTorch, ONNX and TensorRT, it should be `.pt`, `.onnx` and `.plan` format respectively. 

### Creating Model Configuration

Additionally, you might also require a model configuration for each of your model in the model repository. Such a file specifies the platform/framework used, maximum batch sizes, input/output shapes and much more. An example for such a file is shown below for a TensorRT model: 

```txt
name: "resnet50_trt_fp16"
platform: "tensorrt_plan"
dynamic_batching {
preferred_batch_size: [8,16,32]
max_queue_delay_microseconds: 100
}

max_batch_size: 128
input{
  name: "input"
  data_type: TYPE_FP32
  dims: [3,224,224]
}
output{
  name: "output"
  data_type: TYPE_FP32
  dims: 4
}

instance_group [
    {
        count: 1
        kind: KIND_GPU
        gpus: [0]
    }
]
```

Let's talk about all the parameters mentioned above
 - The `name` property is optional. If the name of the model is not specified in the configuration, it is assumed to be the same as the model repository directory containing the model. The value for `platform` depend on the model being used and the different choices can be found [here](https://github.com/triton-inference-server/backend/blob/main/README.md#backends). 
 - Dynamic batching is a feature of Triton that allows inference requests to be combined by the server, so that a batch is created dynamically. The `preferred_batch_size` property indicates the batch sizes that the dynamic batcher should attempt to create. The dynamic batcher can be configured to allow requests to be delayed for a limited time in the scheduler to allow other requests to join the dynamic batch. This time period can be changed using the `max_queue_delay_microseconds` property. 
 - Each model input and output must specify a name, datatype and shape. These are written in the configuration file in a straightforward manner.
 - Triton can provide multiple instances of a model so that multiple inference requests for that model can be handled simultaneously. By default, a single execution instance of the model is created for each GPU available in the system. The instance-group setting can be used to place multiple execution instances of a model on every GPU or on only certain GPUs. These settings can be changed by setting the parameters in `instance_group`.

### Running the server

Once all the configuration and directories are made, you'll be able to deploy your models via the Triton Inference Server. This can easily be done via the following command:

```bash
docker run -it -v <path_to_model_dir>/:/models  -p5000:8000 -p5001:8001 -p5002:8002 nvcr.io/nvidia/tritonserver:21.07-py3 tritonserver --strict-model-config=False --model-repository=/models
```

In order to check whether the server is running, you can send a simple request using:

```bash
curl -v <ip_address>:<port>/v2/health/ready
```

If you get 200 as the return value, then you have successfully deployed the Triton Inference Server. 

### Performing Inference

After you have your model(s) available in Triton, you will want to send inference and other requests to Triton from your client application. There are Python APIs like `tritonclient` which aid this process. Examples for running inference using [HTTP protocol](https://github.com/triton-inference-server/client/blob/main/src/python/examples/simple_http_infer_client.py) and [gRPC protocol](https://github.com/triton-inference-server/client/blob/main/src/python/examples/simple_grpc_infer_client.py) are available too.

### Understanding Model Performance

Understanding and optimising model performance is an important step while deploying models. A critical part of optimising the inference performance of your model is to being able to measure changes in performance as you experiment with different parameters. The `perf_analyzer` application aids us in this process. 

It can setuip inference requests to your model and measure the throughput as well as latency of those requests. To get concrete results, it measures these values over a time window and then repeats until it gets stable measurements. 

A simple command for using `perf_analyzer` is as follows. You just have to specify the model name and it will run inference on it, returning values for latency and throughput

```bash
$ perf_analyzer -m <model_name>
```

You can tweak various parameters like batch size (`-b`), concurrency range (`--concurrency_range`), protocol (`-i`), measurement interval (`-p`), URL ('-u')and many more.
